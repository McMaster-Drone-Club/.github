
# Meeting Summary

Here's a summary of the weekly meeting:

### 1. High-level summary
The team is actively preparing for an upcoming competition, focusing on developing an ML YOLO model for a balloon dataset and planning to conduct simulations in Gazebo.

### 2. Key decisions
*   The team will focus on conducting simulations in Gazebo.

### 3. Risks and blockers
*   Individuals with M-series Macs may face challenges using Gazebo.

### 4. Action items
*   [ ] Mac M-series people who can't use Gazebo can work on other tasks.

---

# AI Research Findings

## Research: Best Cameras for ML Drones

### Concept Explanation

For Machine Learning (ML) drones, the "best" camera is not a single model but rather a type of camera system that provides the necessary data quality and characteristics for effective ML model training and inference. This typically involves considerations beyond basic image resolution, focusing on:

*   **Sensor Type:** CMOS sensors are prevalent due to their lower power consumption and faster readout speeds compared to CCD.
*   **Resolution:** Higher resolution provides more detail, which can be beneficial for object detection and identification at longer ranges or for fine-grained classification. However, it also increases data size and processing requirements.
*   **Frame Rate:** A sufficient frame rate is crucial for capturing dynamic scenes and tracking moving objects without excessive motion blur.
*   **Dynamic Range:** High dynamic range (HDR) allows the camera to capture details in both very bright and very dark areas of a scene simultaneously, which is vital for drones operating in varying light conditions (e.g., flying from bright sunlight into shadows).
*   **Global Shutter vs. Rolling Shutter:** Global shutters capture the entire image at once, preventing distortion (like "jello effect") when the drone or subject is moving rapidly. Rolling shutters capture images line by line, which is less ideal for fast-moving applications but often found in more affordable cameras.
*   **Lens Characteristics:** Field of view (FOV), focal length, and distortion are important for understanding the spatial relationships in the captured data.
*   **Spectral Bands:** While most ML drones use visible light cameras, some applications might benefit from infrared (thermal) or multispectral sensors to detect features not visible to the human eye.
*   **Connectivity and Data Interface:** How the camera connects to the onboard processing unit (e.g., MIPI CSI, USB) affects data transfer speed and power consumption.
*   **Size, Weight, and Power (SWaP):** Drones have strict SWaP constraints, so camera selection must balance performance with these limitations.

### Best Practices

1.  **Define Your ML Task:** The camera choice should directly align with the specific ML task (e.g., object detection, semantic segmentation, anomaly detection, surveying). A drone surveying crops might need high-resolution visible light, while a search and rescue drone might benefit from thermal imaging.
2.  **Consider the Operating Environment:** Factors like lighting conditions (day/night, fog, rain), target distance, and target movement speed will dictate camera requirements.
3.  **Data Acquisition Strategy:** Plan how data will be collected for training and testing. Ensure the camera's output format and quality are suitable for your ML pipeline.
4.  **Onboard Processing Power:** The camera's resolution and frame rate must be manageable by the drone's onboard compute for real-time inference.
5.  **Calibration:** Cameras used for ML tasks, especially those involving spatial analysis or 3D reconstruction, need to be meticulously calibrated (intrinsic and extrinsic parameters).
6.  **Standardization:** Where possible, use cameras with standard interfaces and widely supported drivers to simplify integration and software development.
7.  **Testing and Benchmarking:** Thoroughly test camera performance in simulated and real-world conditions relevant to your drone's mission.

### Recommendations

*   **For General Object Detection/Recognition (e.g., aerial surveillance, inspection):**
    *   **High-Resolution CMOS Sensors:** Look for cameras with resolutions of 4K or higher, providing ample detail.
    *   **Global Shutter:** Essential for minimizing motion blur and distortion in dynamic scenarios.
    *   **Good Dynamic Range:** To handle varying lighting.
    *   **Moderate Frame Rate:** 30-60 fps is often sufficient.
    *   **Examples:**
        *   **FLIR Blackfly S/BFS series:** Popular for industrial applications, known for good image quality and global shutter options.
        *   **Sony IMX series sensors (e.g., IMX250, IMX264, IMX500):** Many camera manufacturers build modules around these high-performance sensors, often offering global shutter.
        *   **Raspberry Pi High Quality Camera (if using a Raspberry Pi compute module):** Offers good resolution and interchangeable lenses, but has a rolling shutter.
*   **For Thermal Imaging (e.g., search and rescue, PV inspection):**
    *   **Uncooled Microbolometer Sensors:** Provide thermal data. Resolution is important, but thermal sensitivity (NETD) is often a more critical metric for detecting small temperature differences.
    *   **Examples:**
        *   **FLIR Boson/Tau 2:** Compact and capable thermal camera cores.
        *   **Seek Thermal:** Offers various thermal camera modules.
*   **For High-Speed Tracking/Analysis:**
    *   **Higher Frame Rates:** 60 fps and above, with global shutter.
    *   **Lower Resolution might be acceptable if frame rate is prioritized.**
*   **For Photogrammetry/3D Mapping:**
    *   **High Resolution:** Critical for detail.
    *   **Global Shutter:** Essential to avoid parallax errors.
    *   **Accurate Distortion Models:** For precise reconstruction.
    *   **Fixed Focal Length Lens:** To ensure consistency.
    *   **Examples:** Higher-end aerial survey cameras from manufacturers like Phase One or Hasselblad (though these are often too large and heavy for typical ML drones). For more accessible options, look at drone-specific mapping cameras.

### Tool Suggestions

*   **OpenCV:** A comprehensive library for computer vision tasks, including image acquisition, processing, and analysis.
*   **ROS (Robot Operating System):** Provides a flexible framework for writing robot software, including camera drivers, image publishers, and subscribers.
*   **Camera SDKs/APIs:** Manufacturers provide software development kits (SDKs) and application programming interfaces (APIs) for controlling their cameras, accessing image data, and configuring settings.
*   **ImageJ/Fiji:** For image analysis and visualization, especially if spectral or detailed pixel-level analysis is needed.
*   **PyTorch/TensorFlow:** For developing and training ML models using the captured image data.

---

## Research: State Diagram for Precision Landing for a Drone

### Concept Explanation

A state diagram is a visual representation of the different operational stages (states) a drone can be in during a precision landing process and the transitions between these states. It helps to define the logic, conditions, and sequence of events required for a safe and accurate landing. Key states often include:

*   **Idle/Hover:** The drone is stable in the air, waiting for a landing command or holding position.
*   **Approaching Target:** The drone is moving towards the designated landing zone.
*   **Target Acquired/Localizing:** The drone has identified the landing target and is actively determining its precise position and orientation relative to it.
*   **Executing Final Approach:** The drone is on a direct trajectory to the landing point, often at a reduced speed and altitude.
*   **On Ground/Landing Complete:** The drone has successfully touched down and all landing systems are secure.
*   **Aborting Landing:** A state triggered if conditions become unsafe or the landing cannot be completed successfully, leading to a transition back to a safe flight mode (e.g., hover or return to home).
*   **Error/Fault:** Indicates a system malfunction preventing normal operation.

Transitions between states are governed by specific events or conditions, such as receiving a landing command, detecting the landing zone, losing target lock, detecting an obstacle, or reaching a predefined altitude/speed.

### Best Practices

1.  **Clearly Define Landing Objectives:** What constitutes "precision"? (e.g., within X cm, on a specific marker).
2.  **Robust Target Detection and Localization:** The system must reliably identify the landing target under various conditions (lighting, weather, occlusions).
3.  **Redundancy:** Consider redundant sensors and algorithms for critical functions like localization and obstacle detection.
4.  **Fail-Safe Mechanisms:** Implement clear abort conditions and safe fallback behaviors (e.g., hover, RTH, emergency landing).
5.  **Smooth Transitions:** Ensure transitions between states are smooth and do not induce instability in the drone's flight.
6.  **Altitude and Speed Control:** Precisely manage altitude and speed during the final approach.
7.  **Environmental Awareness:** Integrate obstacle detection and avoidance during the descent.
8.  **User Feedback:** Provide clear status updates to the operator.

### Recommendations

A typical state diagram for precision landing might include the following states and transitions:

```mermaid
stateDiagram
    [*] --> Idle: Power On / Initialization
    Idle --> ApproachingTarget: Landing Command Received
    ApproachingTarget --> TargetAcquired: Target Detected & Validated
    TargetAcquired --> FinalApproach: Path to Target Confirmed
    FinalApproach --> LandingComplete: Touchdown Detected
    FinalApproach --> AbortingLanding: Obstacle Detected / Target Lost / Unsafe Condition
    TargetAcquired --> AbortingLanding: Target Lost / Unsafe Condition
    ApproachingTarget --> AbortingLanding: Target Not Detected After Timeout / Unsafe Condition

    AbortingLanding --> Hovering: Safe State Reached
    LandingComplete --> Idle: Landing Verified / Disarm
    Idle --> [*]: Shutdown

    state Hovering {
        [*] --> HoveringStable: Maintaining Position
        HoveringStable --> ApproachingTarget: New Landing Command
        HoveringStable --> [*]: Shutdown
    }

    state TargetAcquired {
        [*] --> Localizing: Initial Detection
        Localizing --> Tracking: Position & Orientation Locked
        Tracking --> TargetAcquired: Continuous Lock (Loop back to itself or tracking)
        Tracking --> LostTarget: Target Out of Sensor View
        LostTarget --> AbortingLanding: Unable to Reacquire
        LostTarget --> TargetAcquired: Target Reacquired
    }

    state FinalApproach {
        [*] --> Descending: Controlled Descent Initiated
        Descending --> FinalAdjustments: Fine Tuning Position
        FinalAdjustments --> Touchdown: Initiating Touchdown
        Touchdown --> LandingComplete: Contact Confirmed
        Touchdown --> AbortingLanding: Bounce Detected / Unstable Contact
    }

    state AbortingLanding {
        [*] --> Ascending: Gaining Altitude
        Ascending --> Hovering: Stable Altitude Reached
    }

    state LandingComplete {
        [*] --> Idle: Safe & Stable
    }


    %% Error States (can transition from any state)
    Idle --> Error: System Fault
    ApproachingTarget --> Error: Sensor Failure
    TargetAcquired --> Error: Localization Failure
    FinalApproach --> Error: Motor Malfunction
    AbortingLanding --> Error: Control System Failure
    LandingComplete --> Error: Post-Landing System Check Fail
    Error --> [*]: Manual Intervention Required
```

**Explanation of States:**

*   **`[*]`**: Represents the initial or final state.
*   **`Idle`**: Drone is powered on, systems checked, ready for command.
*   **`ApproachingTarget`**: Drone moves towards the general area of the landing zone.
*   **`TargetAcquired`**: The landing marker/zone is detected and its relative pose is estimated. This state might involve sub-states like `Localizing` and `Tracking`.
*   **`FinalApproach`**: The drone executes the precise descent path to the landing point, often with reduced speed. Sub-states like `Descending`, `FinalAdjustments`, and `Touchdown` are included.
*   **`LandingComplete`**: Drone has successfully touched down and is stable.
*   **`AbortingLanding`**: A safety state triggered by critical events. The drone ascends to a safe hover altitude.
*   **`Hovering`**: A stable flight state, often the fallback for aborted landings or a state to wait for new commands.
*   **`Error`**: Indicates a critical system failure.

**Transitions Explained:**

*   **`Landing Command Received`**: Operator input or autonomous trigger.
*   **`Target Detected & Validated`**: Vision system identifies the landing marker with sufficient confidence.
*   **`Path to Target Confirmed`**: The drone has a clear and safe path for the final descent.
*   **`Touchdown Detected`**: Force sensors or change in altitude indicate contact with the ground.
*   **`Obstacle Detected / Target Lost / Unsafe Condition`**: Any condition that makes continuing the landing risky.
*   **`Safe State Reached`**: Drone is in a stable hover after aborting.
*   **`Landing Verified / Disarm`**: Post-landing checks passed, motors disarmed.
*   **`System Fault / Sensor Failure`**: Malfunctions detected by onboard diagnostics.

### Tool Suggestions

*   **State Machine Libraries:** Many programming languages offer libraries for implementing state machines (e.g., `python-statemachine` in Python, `XState` in JavaScript).
*   **UML Modeling Tools:** Software like Lucidchart, draw.io, PlantUML, or even Mermaid (used above) can be used to create and visualize state diagrams.
*   **Flight Control Software:** Frameworks like PX4 or ArduPilot often have their own internal state management systems that can be customized.

---

## Research: Best Chip to Use on the Drone System (Must Allow High Performance and ML)

### Concept Explanation

Choosing the right chip (System-on-Chip or SoC, or a combination of processors) for a drone system that needs to handle high performance and Machine Learning (ML) involves balancing several critical factors. This chip will be the "brain" of the drone, responsible for flight control, sensor data processing, navigation, and running ML models for tasks like object detection, obstacle avoidance, or computer vision. Key components to consider on such a chip include:

*   **CPU (Central Processing Unit):** Handles general-purpose computing, flight control algorithms, and operating system tasks. Needs to be powerful and efficient.
*   **GPU (Graphics Processing Unit):** Crucial for accelerating ML inference and training, as well as complex image processing tasks.
*   **NPU/TPU/AI Accelerator:** Dedicated hardware specifically designed for high-efficiency, low-power ML inference. This is often the most critical component for on-board ML.
*   **DSP (Digital Signal Processor):** Can be useful for processing sensor data (IMU, GPS, camera streams) efficiently.
*   **Memory Interface:** Bandwidth and capacity for RAM and storage.
*   **Peripherals and I/O:** Connectivity for sensors (cameras, IMU, GPS, lidar), communication modules (Wi-Fi, cellular), and actuators.
*   **Power Efficiency:** Essential for extending flight time on battery power.
*   **SWaP (Size, Weight, and Power):** Constraints are paramount for aerial platforms.

### Best Practices

1.  **Define ML Workload:** Understand the specific ML models you intend to run (e.g., YOLO, SSD, custom CNNs), their size, computational requirements (FLOPS), and latency targets.
2.  **Determine Performance Needs:** What level of real-time processing is required for flight control, sensor fusion, and ML inference?
3.  **Consider the Ecosystem and Toolchain:** The availability of ML frameworks (TensorFlow Lite, PyTorch Mobile, ONNX Runtime), development tools, libraries, and community support is vital.
4.  **Scalability:** Can you start with a lower-end chip and scale up if needed without a complete architectural redesign?
5.  **Power Budget:** Calculate the total power consumption expected from the compute module to ensure it fits within the drone's battery and thermal limits.
6.  **Data Throughput:** Ensure the chip and its interfaces can handle the data rates from sensors, especially high-resolution cameras.
7.  **Cost-Effectiveness:** Balance performance requirements with budget constraints.
8.  **Availability and Lead Times:** For production, consider the commercial availability and supply chain of the chosen chip.

### Recommendations

The landscape of high-performance drone chips is rapidly evolving, with several key players offering powerful solutions. Here are some top recommendations, categorized by their typical use cases:

1.  **For Extreme Performance and Complex ML (e.g., advanced perception, autonomous navigation):**

    *   **NVIDIA Jetson Platform (e.g., Jetson AGX Orin, Jetson Xavier NX):**
        *   **Concept:** These are powerful embedded computing boards designed for AI, robotics, and edge computing. They integrate a high-performance CPU, a very capable GPU, and often dedicated AI accelerators (DLAs).
        *   **Pros:** Unmatched ML performance for embedded systems, mature CUDA and cuDNN support, extensive ecosystem (JetPack SDK, ROS integration, cloud integration). Excellent for complex vision tasks.
        *   **Cons:** Higher power consumption, larger SWaP footprint compared to simpler solutions. Can be overkill for simpler tasks.
        *   **Best Use:** Autonomous flight, real-time object tracking with high accuracy, complex sensor fusion, AI-powered navigation.

2.  **For Balanced Performance and Power Efficiency (e.g., object detection, obstacle avoidance):**

    *   **Qualcomm Flight Platform (e.g., Qualcomm Flight RB5, Snapdragon 8 Gen 1/2 based solutions):**
        *   **Concept:** Qualcomm offers integrated SoCs designed for drones and robotics, often derived from their mobile chipsets. They typically include a strong CPU, GPU, and dedicated AI engines (Hexagon DSP/NPU).
        *   **Pros:** Excellent integration of CPU, GPU, and AI accelerators, good power efficiency, strong connectivity options (Wi-Fi, 5G), well-suited for mobile robotics.
        *   **Cons:** Ecosystem can be more closed than NVIDIA's, specific drone platforms may have limited availability or customization options.
        *   **Best Use:** Smart camera drones, advanced waypoint navigation with obstacle avoidance, visual-inertial odometry (VIO).

    *   **NXP i.MX 8/9 Series (e.g., i.MX 8X, i.MX 93):**
        *   **Concept:** These are powerful application processors with heterogeneous computing cores, often including ARM Cortex-A CPUs, Cortex-M MCUs for real-time tasks, and sometimes integrated GPUs or neural processing units (NPUs).
        *   **Pros:** Flexible architecture, good balance of performance and power, strong industrial pedigree, good for embedded vision and moderate ML.
        *   **Cons:** Requires more integration effort than a dedicated drone platform; ML performance might not match the top-tier NVIDIA or Qualcomm solutions without external accelerators.
        *   **Best Use:** Flight controllers requiring onboard vision processing, sensor fusion, and moderate ML inference.

3.  **For Cost-Effective ML Inference with Moderate Performance:**

    *   **Raspberry Pi Compute Module 4 (with I/O board):**
        *   **Concept:** A very popular, low-cost ARM-based single-board computer. While not a specialized AI chip, its Broadcom processor has a VideoCore VI GPU capable of some ML acceleration, and it can interface with external AI accelerators.
        *   **Pros:** Extremely affordable, vast community support, good for prototyping and educational purposes, flexible I/O.
        *   **Cons:** Limited raw ML performance compared to dedicated solutions, not ideal for very demanding real-time ML tasks without augmentation.
        *   **Best Use:** Simple object detection, basic computer vision tasks, prototyping ML algorithms on a drone, or acting as a co-processor alongside a dedicated flight controller.

    *   **Any Single Board Computer (SBC) with a dedicated AI Accelerator:**
        *   **Concept:** Pairing a capable SBC (like a higher-end Raspberry Pi, or an Intel NUC) with a USB or M.2 AI accelerator (e.g., Google Coral USB Accelerator, Intel Movidius Neural Compute Stick).
        *   **Pros:** Allows for adding powerful ML inference capabilities to existing drone systems without replacing the main compute unit. Cost-effective way to boost ML performance.
        *   **Cons:** Adds complexity, can increase power consumption, and may have latency issues depending on the connection.
        *   **Best Use:** Adding ML capabilities to existing drone platforms, upgrading performance for specific ML tasks.

### Tool Suggestions

*   **ML Frameworks:**
    *   **TensorFlow Lite:** Optimized for mobile and embedded devices.
    *   **PyTorch Mobile:** For deploying PyTorch models on edge devices.
    *   **ONNX Runtime:** An inference engine that supports models from various frameworks.
*   **Hardware SDKs and Libraries:**
    *   **NVIDIA JetPack SDK:** For Jetson platforms, includes CUDA, cuDNN, TensorRT, and AI libraries.
    *   **Qualcomm Snapdragon® Neural Processing Engine (NPE):** For optimizing and running ML models on Qualcomm hardware.
    *   **NXP eIQ™ ML Software Development Environment:** For deploying ML models on NXP processors.
    *   **Google Coral SDK:** For working with Coral AI accelerators.
*   **Operating Systems:**
    *   **Linux (Ubuntu, Yocto Project):** The most common OS for embedded AI and robotics.
    *   **RTOS (Real-Time Operating System):** For critical flight control tasks where determinism is paramount.
*   **Development Boards and Evaluation Kits:** Most chip manufacturers offer development boards that make it easier to prototype and test their silicon.
*   **Benchmarking Tools:** Tools to measure inference speed, power consumption, and accuracy of ML models on the target hardware.
